---
title       : Developing Data Products Project
subtitle    : Predicting spam
author      : Nicholas Tang
job         : Student
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : solarized_light      # 
widgets     : []            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
github:
  user: nicholas-yjtang
  repo: developingdataproducts_slidify
--- .changetitlecolor

## Introduction

> * Our application will predict the text input is spam or nonspam.  
  
> * The data to build the prediction model is provided by [Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Spambase)  
  
> * We partition the data randomly, using 60% as the training, and 40% as the test/validation set  
    
> * The prediction model is built using the caret library, with focus on the boosted tree algorithm, with k-fold cross validation (k=10)  
  
> * After creating the model. the application will take the text input and will convert it into the same format as the data set as described by [Machine Learning Repository - Dataset Description](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names)  
  
> * The final result is obtained by running the converted input through our prediction model, and displays if it is spam or not

--- .changetitlecolor

## Prediction model

The following is the code that creates the prediction mode  

```{r, warning=FALSE, message=FALSE,cache=TRUE}
  require(kernlab)
  require(caret)  
  require(e1071)
  require(gbm)
  data(spam)
  set.seed(12345)
  inTrain <- createDataPartition(y=spam$type,p=0.6,list=FALSE)
  training <- spam[inTrain,]
  testing <- spam[-inTrain,]
  fitControl <- trainControl(method="cv", number=10)
  modelFit <- train(type ~., data=training, method="gbm", 
                    trControl = fitControl, verbose=FALSE) 

```

--- .changetitlecolor

## Performance (Training and fitting)

We plot the model tuning parameters and the performance estimate process to see how our parameters compare  

```{r, message=F, warning=F, cache=FALSE, echo=FALSE}
plot(modelFit)
```

--- .changetitlecolor

## Performance (Validation)

We measure the performance of our algorithm, testing the model with our remaining test set (40%). The below describes the results in a confusion matrix table  
  
```{r, message=F, warning=F, results='asis', echo=FALSE}
prediction <- predict(modelFit, testing)
prediction_results <- confusionMatrix(prediction, testing$type)
require(pander)
pander(prediction_results$table, style='rmarkdown')
```
  
  
The out of sample accuracy of our model is **`r prediction_results$overall["Accuracy"]`**  
  
Our application is reasonably accurate (out of sample performance > 90%)


